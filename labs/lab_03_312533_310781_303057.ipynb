{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022c879b-e00c-468f-854f-aa9c798796b9",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Lab 3 ‒  Classification\n",
    "\n",
    "\n",
    "**Group ID:** 42\n",
    "\n",
    "**Author 1 (sciper):** Orélian Kohler (312533)  \n",
    "**Author 2 (sciper):** Quang Long Ho Ngo (310781)  \n",
    "**Author 3 (sciper):** Caroline Legrand (303057)\n",
    "\n",
    "**Release date:** 19.04.2024  \n",
    "**Due date:** 03.05.2024 (11:59 pm)\n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The lab assignments are designed to teach practical implementation of the topics presented during class as well as preparation for the final project, which is a practical project that ties together the topics of the course.\n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external\n",
    "functions from image processing/ML libraries like OpenCV and sklearn as long as there is sufficient explanation\n",
    "in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook <font color='red'> rerun </font>the notebook from scratch !**\n",
    "`Kernel` > `Restart & Run All`\n",
    "\n",
    "TAs should be able to rerun your code end to end without having any issues. If not, you might lose part of the points during grading.\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7e88e80-cbae-4f05-ada3-cc46c0efbc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/bluedrack/.local/lib/python3.9/site-packages (3.4.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/bluedrack/.local/lib/python3.9/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/bluedrack/.local/lib/python3.9/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: pillow in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (10.2.0)\n",
      "Requirement already satisfied: pandas in /home/bluedrack/.local/lib/python3.9/site-packages (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from scikit-learn) (3.4.0)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install pillow\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20b7150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check is at least python 3.9\n",
    "import sys \n",
    "assert (sys.version_info.major == 3) and (sys.version_info.minor == 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91da76e6-851c-4cc7-852b-fcedac869495",
   "metadata": {},
   "source": [
    "Please take note that PyTorch will be utilized in this lab. PyTorch is a widely recognized library for deep learning. Prior to commencing the lab, we kindly ask you to review this quick tutorial available [here](https://pytorch.org/tutorials/beginner/basics/intro.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8df3391-feb0-47b0-b9f9-50eea0e802d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Linux\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.8.1+cpu in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (1.8.1+cpu)\n",
      "Requirement already satisfied: torchvision==0.9.1+cpu in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (0.9.1+cpu)\n",
      "Requirement already satisfied: typing-extensions in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from torch==1.8.1+cpu) (4.9.0)\n",
      "Requirement already satisfied: numpy in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from torch==1.8.1+cpu) (1.26.4)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages (from torchvision==0.9.1+cpu) (10.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "# Get os name\n",
    "os_name = platform.system().lower()\n",
    "num_workers = 8\n",
    "\n",
    "# OS X\n",
    "if 'darwin' in os_name:\n",
    "    print(\"Detected OS X\")\n",
    "    %pip install torch==1.8.1 torchvision==0.9.1 torchaudio==0.8.1\n",
    "# Linux \n",
    "elif 'linux' in os_name:\n",
    "    print(\"Detected Linux\")\n",
    "    %pip install torch==1.8.1+cpu torchvision==0.9.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
    "# Windows \n",
    "else:\n",
    "    print(\"Detected Windows\")\n",
    "    num_workers = 0  # Hard fix for Windows users\n",
    "    %pip install torch==1.8.1+cpu torchvision==0.9.1+cpu -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e397661f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import main packages\n",
    "import os\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Callable\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.covariance import LedoitWolf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30adfc3",
   "metadata": {},
   "source": [
    "# Real-World Image Classification in Histopathology (30 points)\n",
    "\n",
    "Supervised learning for classifying histopathology images, despite its capabilities, faces significant challenges. A primary obstacle is its reliance on labeled data, which is often scarce and costly to acquire due to the need for expert annotations. This scarcity can impede model performance, particularly when dealing with uncommon diseases or subtle pathological patterns. Moreover, supervised models may struggle with generalization to unseen data or variations in tissue staining protocols. \n",
    "\n",
    "This lab will concentrate on constructing classification models tailored for histopathology using the least labeled data possible. Since the lab does not center on computing descriptors for the images, only image features will be provided. Specifically, we utilized [CTransPath](https://github.com/Xiyue-Wang/TransPath), one of the most robust and precise existing image feature extractors in histopathology, as the descriptor. Your focus will be on classifying these features for downstream tasks crucial to histopathological analysis.\n",
    "\n",
    "Before starting, make sure the data are located as follows:\n",
    "```code\n",
    "├── lab_03_iapr.ipynb\n",
    "└── data\n",
    "    └── data_lab_03\n",
    "        ├── part_01\n",
    "        │    ├── k16_train.pth\n",
    "        │    ├── k16_val.pth\n",
    "        │    ├── k16_test.pth\n",
    "        │    └── k16.svg\n",
    "        └── part_02\n",
    "            ├── acinar.png\n",
    "            ├── solid.png\n",
    "            ├── wsi.png\n",
    "            ├── DHMC_0001.png\n",
    "            └── DHMC_0007.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b29c863",
   "metadata": {},
   "source": [
    "## Part 1 - Tumor, Stroma Classification in Colorectal Cancer Histopathology (11 points)\n",
    "\n",
    "Colorectal cancer ranks among the most prevalent cancers affecting both men and women. Accurate diagnosis, supplemented with prognostic and predictive biomarker information, plays a pivotal role in patient monitoring and facilitating personalized treatment approaches. One crucial biomarker is the Tumor/Stroma ratio (TSR) observed in unhealthy colon tissues. This ratio serves as an indicator of cancer invasiveness, with higher ratios correlating to increased invasiveness and, consequently, diminished patient survival probabilities.\n",
    "\n",
    "Traditionally, pathologists assess the TSR by visually inspecting unhealthy tissue samples under a microscope, relying on their expertise to estimate the ratio. However, given the large volume of samples and the occasional lack of precision in estimations, there arises a pressing need for automated recognition of various tissue types within histological images. The development of a multi-class classifier becomes imperative to accurately identify the diverse tissue types present. Typically, these tissue types include TUMOR, STROMA, LYMPHO (lymphocytes), MUCOSA, COMPLEX (complex stroma), DEBRIS, ADIPOSE, and EMPTY (background).\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"../data/data_lab_03/part_01/k16.svg\" width=\"1100\">\n",
    "    <figcaption>Fig1: Collection of tissue types in colorectal cancer histology.</figcaption>\n",
    "</figure>\n",
    "<br />\n",
    "\n",
    "Until now, state-of-the-art methodologies in histology have predominantly relied on deep-learning-based supervised learning techniques. However, a significant drawback of such an approach lies in the requirement for access to a meticulously annotated training dataset. Annotating histological data poses considerable challenges—it is a time-consuming process that demands the expertise of pathologists. Moreover, annotators are compelled to label every tissue type, even though only two (TUMOR and STROMA) are of primary interest.\n",
    "\n",
    "To address these challenges, we propose an alternative approach. To streamline the annotation process, we task the annotator with labeling only the tissues of interest (TUMOR and STROMA), and discarding the rest. Subsequently, we aim to train a binary classifier capable of automatically recognizing these specific tissues during testing. This novel approach seeks to alleviate the burden of annotation while still enabling accurate identification of the critical tissue types. In this section, we will proceed with implementing and evaluating this proposed methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4a6936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of data in train set\n",
      "#Tumor examples: 439\n",
      "#Stroma examples: 439\n",
      "\n",
      "Distribution of data in validation set\n",
      "#Tumor examples: 93\n",
      "#Stroma examples: 93\n"
     ]
    }
   ],
   "source": [
    "dataroot = \"../data/data_lab_03/part_01\"\n",
    "\n",
    "# train features and labels\n",
    "train_data = torch.load(os.path.join(dataroot, \"k16_train.pth\"))\n",
    "train_x, train_y = train_data[\"features\"], train_data[\"labels\"]\n",
    "\n",
    "# validation features and labels\n",
    "val_data = torch.load(os.path.join(dataroot, \"k16_val.pth\"))\n",
    "val_x, val_y = val_data[\"features\"], val_data[\"labels\"]\n",
    "\n",
    "print(\"Distribution of data in train set\")\n",
    "print(\"#Tumor examples: {}\".format(len(train_y[train_y == 0])))\n",
    "print(\"#Stroma examples: {}\".format(len(train_y[train_y == 1])))\n",
    "\n",
    "print(\"\\nDistribution of data in validation set\")\n",
    "print(\"#Tumor examples: {}\".format(len(val_y[val_y == 0])))\n",
    "print(\"#Stroma examples: {}\".format(len(val_y[val_y == 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002cf19f",
   "metadata": {},
   "source": [
    "### 1.1 Binary classifier with Mahalanobis distance (2.5 pts)\n",
    "\n",
    "Your task is to construct this binary classifier utilizing the Mahalanobis distance as taught in class. Begin by executing the cell below to load the training and validation features for TUMOR and STROMA. These features have been computed using a self-supervised model tailored for histopathology, known as CTransPath. Note that label `0` corresponds to TUMOR and label `1` to STROMA:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec65d58",
   "metadata": {},
   "source": [
    "* **Q1 (1 pt)**: Complete the `fit` method in `MahalanobisClassifier`. This method calculates the parameters necessary for the Mahalanobis Classifier when fitted to the training data.\n",
    "* **Q2 (1 pt)**: Complete the `predict` method in `MahalanobisClassifier`. This method is responsible for predicting the class for each test feature as well as the distance to class means using the Mahalanobis distance method.\n",
    "\n",
    "**Note**: It is forbidden to use any prebuilt Mahalanobis distance function. You may only use `LedoitWolf` in `sklearn.covariance` for computing a stable covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e6976bb7-f856-4980-9268-fc66be80dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MahalanobisClassifier:\n",
    "    \"\"\"Mahalanobis based classifer\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            means (torch.tensor): (n_classes, d) Mean of the features for each class\n",
    "            inv_covs (torch.tensor): (n_classes, d, d) Inverse of covariance matrix across d features for each class   \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.means = None\n",
    "        self.inv_covs = None\n",
    "        \n",
    "    def fit(self, train_x : torch.Tensor, train_y : torch.Tensor):\n",
    "        \"\"\"Computes parameters for Mahalanobis Classifier (self.mean and self.cov), fitted on the training data.\n",
    "\n",
    "        Args:\n",
    "            train_x (torch.Tensor): (N, d) The tensor of training features\n",
    "            train_y (torch.Tensor): (N,) The tensor of training labels\n",
    "        \"\"\"\n",
    "\n",
    "        # Define number of classes\n",
    "        n_classes = len(np.unique(np.unique(train_y)))\n",
    "        n, d = train_x.shape\n",
    "        \n",
    "        # Set default values\n",
    "        means = torch.zeros((n_classes, d), dtype=train_x.dtype)\n",
    "        inv_covs = torch.ones((n_classes, d, d), dtype=train_x.dtype)\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        for c in range(n_classes):\n",
    "            idx = train_y == c\n",
    "            samples = train_x[idx]\n",
    "            means[c] = samples.mean(dim=0)\n",
    "            inv_covs[c] = torch.tensor(np.linalg.inv(np.cov(samples.T)))\n",
    "        \n",
    "\n",
    "        self.means = means\n",
    "        self.inv_covs = inv_covs\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, test_x : torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predicts the class of every test feature, using the Mahalanobis Distance\n",
    "\n",
    "        Args:\n",
    "            test_x (torch.Tensor): (N, d) The tensor of test features\n",
    "\n",
    "        Returns:\n",
    "            preds (torch.Tensor): (N,) The predictions tensor (id of the predicted class {0, 1, ..., n_classes-1})\n",
    "            dists (torch.Tensor): (N, n_classes) Mahalanobis distance from sample to class means\n",
    "        \"\"\"\n",
    "\n",
    "        # Define default output value\n",
    "        N, d = test_x.shape\n",
    "        dists = torch.zeros((N, self.means.shape[0]), dtype=test_x.dtype)\n",
    "        preds = torch.zeros(N, dtype=test_x.dtype)\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        for i in range(N):\n",
    "            for c in range(self.means.shape[0]):\n",
    "                diff = test_x[i] - self.means[c]\n",
    "                dists[i, c] = torch.sqrt(torch.dot(torch.matmul(self.inv_covs[c],diff), diff))\n",
    "            preds[i] = torch.argmin(dists[i])\n",
    "\n",
    "        return preds, dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "eeb74ba5-f0cf-4d35-ab3a-ea5427292363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mahalanobis_classifier(\n",
    "    fa: torch.Tensor, fb: torch.Tensor, y: torch.Tensor, cls_name: list[str], colors: list[str], title: str):\n",
    "    \"\"\"Display Mahalanobis distances for the first two features over samples\n",
    "\n",
    "    Args:\n",
    "        fa (torch.Tensor): (N,) First feature component\n",
    "        fb (torch.Tensor): (N,) Second feature component\n",
    "        y (torch.Tensor): (N,) Class ground truth\n",
    "        cls_name (list of str): (n_classes,) Name of classes as list\n",
    "        title (str): Title of plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create plot\n",
    "    _, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "    # Plot results\n",
    "    for i, c in enumerate(np.unique(y)):\n",
    "        ax.scatter(fa[y == c], fb[y == c], marker=\"o\", c=colors[i], label=\"{}\".format(cls_name[i]))\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel(\"Distance to Tumor\")\n",
    "    ax.set_ylabel(\"Distance to Stroma\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d5f505",
   "metadata": {},
   "source": [
    "* **Q3 (0.5 pt)**: After fitting your classifier on the training data, compute the accuracy of the validation data. Are you satisfied with the results? Your results should be above (97%).\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "96687c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create classifier\n",
    "classifier = MahalanobisClassifier()\n",
    "\n",
    "# Fit to train data\n",
    "classifier.fit(train_x, train_y)\n",
    "\n",
    "# Apply on validation data and compute accuracy\n",
    "val_y_hat, val_y_dist = classifier.predict(val_x)\n",
    "accuracy = accuracy_score(val_y, val_y_hat)\n",
    "\n",
    "# Plot results (distance to means and prediction of classes)\n",
    "plot_mahalanobis_classifier(\n",
    "    fa=val_y_dist[:,0] , fb=val_y_dist[:,1], y=val_y,\n",
    "    cls_name=[\"Tumor\", \"Stroma\"], colors=[\"r\", \"b\"],\n",
    "    title=\"Mahalanobis distances for samples\\nValidation set accuracy: {:.2f}%\".format(100*accuracy),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96c344",
   "metadata": {},
   "source": [
    "### 1.2 Out-of-Distribution detection with Mahalanobis distance (3.5 pts)\n",
    "\n",
    "Your classifier appears to perform well. However, during testing, it's possible for other tissue types to be present, which cannot be manually filtered out. Moreover, these tissue types may not be recognized by the model as they fall outside the labeled training distribution (It is the consequence of the laziness of the annotators ;)). Therefore, it's crucial to filter out these out-of-distribution (OoD) samples.\n",
    "\n",
    "One approach to OoD detection involves computing an OoD-ness score for each test example. This score should be low for in-distribution (ID) examples and high for OoDs. Subsequently, a threshold is defined, for which any example with a greater OoD-ness is discarded, while those below it are forwarded to the model for prediction. An example of an OoD-ness score is the minimum Mahalanobis distance to means."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0140d67",
   "metadata": {},
   "source": [
    "* **Q1 (0.5 pt)**: Why do you think the minimum Mahalanobis distance is a good OoD-ness score?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f87ad75-97a9-4937-8f92-beaf0f8edda1",
   "metadata": {},
   "source": [
    "Start by running the cell below to load the test set. It comprises TUMOR and STROMA samples, along with other tissue types. Note that OoD tissues types are labeled to `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3a7a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torch.load(os.path.join(dataroot, \"k16_test.pth\"))\n",
    "test_x, test_y = test_data[\"features\"], test_data[\"labels\"]\n",
    "\n",
    "# Display distribution of test samples\n",
    "print(\"Distribution of data in test set\")\n",
    "print(\"#Tumor examples: {}\".format(len(test_y[test_y == 0])))\n",
    "print(\"#Stroma examples: {}\".format(len(test_y[test_y == 1])))\n",
    "print(\"#OoD examples: {}\".format(len(test_y[test_y == -1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffad9ca",
   "metadata": {},
   "source": [
    "* **Q2 (0.5 pt)**: We create a new classifier `MahalanobisOODClassifier` that inherits from the previous one. Reimplement the function `predict` such that it returns as well the OoD scores. We define the `ood_scores` as the minimum Mahalanobis distance from the classifier. Your accuracy results should be the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f20bd6-ca9e-4656-bd97-49fc1a1506a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MahalanobisOODClassifier(MahalanobisClassifier):\n",
    "    \"\"\"Predicts the class of every test feature, using the Mahalanobis Distance\n",
    "\n",
    "    Args:\n",
    "        test_x (torch.Tensor): (N x d) The tensor of test features\n",
    "\n",
    "    Returns:\n",
    "        preds (torch.Tensor): (N,) The predictions tensor (id of the predicted class {0, 1, ..., n_classes-1})\n",
    "        dists (torch.Tensor): (N, n_classes) Mahalanobis distance from sample to class means\n",
    "        ood_scores (torch.Tensor): (N,) Score of OoDness as the minimal distance from the sample to classes\n",
    "    \"\"\"\n",
    "\n",
    "    def predict(self, test_x : torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        # Get super prediction (from MahalanobisClassifier)\n",
    "        preds, dists = super().predict(test_x=test_x)\n",
    "        N = preds.shape[0]\n",
    "\n",
    "        # Assign dummy values to scores\n",
    "        ood_scores = np.zeros(N)\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        \n",
    "        return preds, dists, ood_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e77963-327a-4e3c-addb-df313f4550fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mahalanobis_ood_classifier(\n",
    "    fa: torch.Tensor, fb: torch.Tensor, ood_score: torch.Tensor, cls_name: list[str], title: str):\n",
    "    \"\"\"Display Mahalanobis distances for the first two features over samples as well as OoD scores.\n",
    "\n",
    "    Args:\n",
    "        fa (torch.Tensor): (N,) First feature component\n",
    "        fb (torch.Tensor): (N,) Second feature component\n",
    "        ood_score (torch.Tensor): (N,) OoDness of samples\n",
    "        cls_name (list of str): (n_classes,) Name of classes as list\n",
    "        title (str): Title of plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "    # Plot results\n",
    "    pcm = ax.scatter(fa, fb, c=ood_score, marker=\"o\", label=\"OoD score\")\n",
    "    \n",
    "    # Labels\n",
    "    ax.set_xlabel(\"Distance to Tumor\")\n",
    "    ax.set_ylabel(\"Distance to Stroma\")\n",
    "    fig.colorbar(pcm, ax=ax, label=\"OoD score\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb72c271-a0d2-4e25-bcb3-31d8019a18f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classifier\n",
    "classifier_ood = MahalanobisOODClassifier()\n",
    "\n",
    "# Fit to train data\n",
    "classifier_ood.fit(train_x, train_y)\n",
    "\n",
    "# Apply on validation data and compute accuracy\n",
    "val_y_hat, val_y_dist, val_y_ood_scores = classifier_ood.predict(val_x)\n",
    "accuracy = accuracy_score(val_y, val_y_hat)\n",
    "\n",
    "# Plot results (distance to means and prediction of classes)\n",
    "plot_mahalanobis_ood_classifier(\n",
    "    fa=val_y_dist[:,0] , fb=val_y_dist[:,1], ood_score=val_y_ood_scores,\n",
    "    cls_name=[\"Tumor\", \"Stroma\"],\n",
    "    title=\"OoD scores for samples\\nValidation set accuracy: {:.2f}%\".format(100*accuracy),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d56315-272b-4c5e-ba61-8a2e0f6c25c4",
   "metadata": {},
   "source": [
    "* **Q3 (0.5 pt)**: Based on the validation set OoD scores, determine a threshold for the minimum Mahalanobis distance such that 95% of the validation samples are identified as ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aeb80fc-90f5-49d5-ae8d-b579a237ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ood_threshold(ood_scores, quantile=0.95):\n",
    "    \"\"\" Get OoD threshold based on measured scores and quantile\n",
    "\n",
    "    Args:\n",
    "        ood_scores (torch.Tensor): (N, ) N measured OoDness scores\n",
    "        quantile (float): Percentage of samples that are considered as in distribution\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default value\n",
    "    threshold = 0\n",
    "\n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb771f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get threshold\n",
    "q = 0.95\n",
    "threshold_val = get_ood_threshold(ood_scores=val_y_ood_scores, quantile=q)\n",
    "\n",
    "print(\"Validation threshold {:.0f}% = {:.2f}\".format(100*q, threshold_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bb2b8-3e34-4805-b982-bdb68ae558d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ood_scores(ood_scores: torch.Tensor, threshold: float):\n",
    "    \"\"\" Plot OoD scores and the threshold to quantile.\n",
    "\n",
    "    Args:\n",
    "        ood_scores (torch.Tensor): (N, ) N measured OoDness scores\n",
    "        threshold (float): OoD threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.hist(ood_scores[ood_scores <= threshold], bins=20, label=\"ID\")\n",
    "    plt.hist(ood_scores[ood_scores > threshold], bins=20, label=\"OoD\")\n",
    "    plt.vlines(threshold, ymin=0, ymax=10, color='k', ls='--', label=\"Threshold\")\n",
    "    plt.xlabel(\"OoD scores\")\n",
    "    plt.ylabel(\"Score density\")\n",
    "    plt.title(\"OoD scores and threshold\")\n",
    "    plt.legend()\n",
    "\n",
    "# Plot ood scores and threshold\n",
    "plot_ood_scores(ood_scores=val_y_ood_scores, threshold=threshold_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102b92ae",
   "metadata": {},
   "source": [
    "* **Q4 (2 pts)**: Complete the function `compute_metrics` that computes the recall for TUMOR, STROMA, and OoD examples as well as the average recall over the 3 classes. To do so, you need to consider OoDs as a third class by assigning the prediction `-1` to filtered-out examples based on your threshold. Based on your results, conclude on the feasibility of the proposed pipeline. Propose a solution that would require the least annotation possible but that could significantly increase your OoD recall. Both Tumor and Stroma recall should be above 90%. Recall for OOD should be above 80%.\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fdfb02-a3f5-4f72-86b4-4399db6cf8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y, y_hat, ood_scores, threshold):\n",
    "    \"\"\" Compute recall for tumor, stroma, and OoD as well as the average recall.\n",
    "\n",
    "    Args:\n",
    "        y (torch.Tensor): (N) Class ground truth {-1, 0, 1, ..., n_classes}\n",
    "        y_hat (torch.Tensor): (N,) Class predictions {0, 1, ..., n_classes}\n",
    "        ood_scores (torch.Tensor): (N, ) N measured OoDness scores\n",
    "        threshold (float): OoD threshold\n",
    "    \"\"\"\n",
    "    # Define variable with dummy values \n",
    "    recall_tumor = 0\n",
    "    recall_stroma = 0\n",
    "    recall_ood = 0\n",
    "    avg_recall = 0\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return recall_tumor, recall_stroma, recall_ood, avg_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb4b7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set\n",
    "test_y_hat, test_y_dist, test_y_ood_scores = classifier_ood.predict(test_x)\n",
    "\n",
    "# Compute metrics\n",
    "recall_tumor, recall_stroma, recall_ood, avg_recall = compute_metrics(\n",
    "    y=test_y, y_hat=test_y_hat, ood_scores=test_y_ood_scores, threshold=threshold_val)\n",
    "\n",
    "# Display metrics\n",
    "print(f\"Tumor recall: {recall_tumor*100:.2f}%\")\n",
    "print(f\"Stroma recall: {recall_stroma*100:.2f}%\")\n",
    "print(f\"OoD recall: {recall_ood*100:.2f}%\")\n",
    "print(f\"Average recall: {avg_recall*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be4988-7bed-4869-9bfc-ffc945061335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the classification with OoD and features\n",
    "plot_mahalanobis_classifier(\n",
    "    fa=test_y_dist[:,0] , fb=test_y_dist[:,1], y=test_y,\n",
    "    cls_name=[\"OoD\", \"Tumor\", \"Stroma\"], colors=[\"k\", \"r\", \"b\"],\n",
    "    title=\"Mahalanobis distances for samples (Ground Truth)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82232f37-69e9-4de1-bbd2-8e2801bf3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the classification with OoD and features\n",
    "plot_mahalanobis_classifier(\n",
    "    fa=test_y_dist[:,0] , fb=test_y_dist[:,1], y=test_y_hat,\n",
    "    cls_name=[\"OoD\", \"Tumor\", \"Stroma\"], colors=[\"k\", \"r\", \"b\"],\n",
    "    title=\"Mahalanobis distances for samples (Prediciton)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2442b2d",
   "metadata": {},
   "source": [
    "### 1.3 Out-of-Distribution detection with k-NN (5 pts)\n",
    "\n",
    "Let's explore another technique based on k-Nearest Neighbors (k-NN). The features utilized have been extracted from a self-supervised model, known for their efficacy as k-NN classifiers. This motivates us to implement a k-NN classifier for identifying TUMOR and STROMA. Additionally, the k-NN distance serves as a suitable OoD-ness score, aligning well with our task requirements.\n",
    "\n",
    "* **Q1 (2 pts)**: Complete the `fit` and `predict` functions in `kNNClassifier`. Also, assign your own handcrafted OoD score in `predict`. We recommend to use `top_k` function from torch ([doc](https://pytorch.org/docs/stable/generated/torch.topk.html)). When using k>1, use majority voting to select the winning class.\n",
    "\n",
    "**Note**: It is forbidden to use any prebuilt k-NN classifier function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d39391-e446-47ca-836c-c49d8f0ed9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class kNNClassifier:\n",
    "    \"\"\"k-NN based classifier\"\"\"\n",
    "\n",
    "    def __init__(self, k : int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            k (int): The number of neighbors to consider for the classification\n",
    "            features (torch.Tensor): (N, d) feature of the N train samples\n",
    "            labels (torch.Tensor): (N,) labels for train samples\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "        self.features = None\n",
    "        self.labels = None\n",
    "\n",
    "    def fit(self, train_x : torch.Tensor, train_y : torch.Tensor):\n",
    "        \"\"\"Store training data parameters (features and labels) for k-NN classifier.\n",
    "\n",
    "        Args:\n",
    "            train_x (torch.Tensor): (N, d) The tensor of training features\n",
    "            train_y (torch.Tensor): (N,) The tensor of training labels\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get size and default values\n",
    "        N, d = train_x.shape\n",
    "        features = torch.zeros((N, d))\n",
    "        labels = torch.zeros(N)\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def predict(self, test_x : torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predicts the class of every test feature, using the k-NN\n",
    "\n",
    "        Args:\n",
    "            test_x (torch.Tensor): (N x d) The tensor of test features\n",
    "\n",
    "        Returns:\n",
    "            preds (torch.Tensor): (N,) The tensor of class predictions {0, 1, ..., n_classes}\n",
    "            ood_scores (torch.Tensor): (N,) The OoD score predictions\n",
    "        \"\"\"\n",
    "\n",
    "                \n",
    "        # Get size and default values\n",
    "        N, d = test_x.shape\n",
    "        preds = torch.zeros(N)\n",
    "        ood_scores = torch.zeros(N)\n",
    "\n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        \n",
    "        return preds, ood_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40473834",
   "metadata": {},
   "source": [
    "* **Q2 (1 pt)**: Find the best `k` among `[1, 3, 5, 9, 15, 25]` based on the validation set. What is the best `k` and accuracy?  Your results should be above (97%).\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5c1f3-95b6-4e11-adee-fe8b95faaaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best k for knn fitting (to find among suggested ks)\n",
    "ks = [1, 3, 5, 9, 15, 25]\n",
    "best_k = 0\n",
    "best_accuracy = 0.\n",
    "\n",
    "# Iterate over ks\n",
    "for k in ks:\n",
    "\n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    continue\n",
    "\n",
    "# Print best K\n",
    "print(f\"\\nBest @ k: {best_k} -> {best_accuracy*100:.2f}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e07e16-0983-4139-8e57-6fd2e8c33b33",
   "metadata": {},
   "source": [
    "* **Q3 (1 pt)**: Compute the threshold such that 95% of validation samples are detected as ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3b5ca6-48c4-4d9b-a801-9174ce7e5f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best threshold\n",
    "threshold_val = 0\n",
    "# Preidcted val ood scores\n",
    "val_y_ood_scores = torch.zeros(len(val_y))\n",
    "\n",
    "# ------------------\n",
    "# Your code here ... \n",
    "# ------------------\n",
    "\n",
    "print(\"Validation threshold {:.0f}% = {:.2f}\".format(100*q, threshold_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c61cc2-dee3-4f71-8416-96a2decaeed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ood scores and threshold\n",
    "plot_ood_scores(ood_scores=val_y_ood_scores, threshold=threshold_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f9d26-6d43-4bf9-b69f-64e679f3d527",
   "metadata": {},
   "source": [
    "* **Q4 (1 pts)**: We evaluate your classifier on the test set like in `1.2 Q4`. Is it better than Mahalanobis distance? Why?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543bcc55-b63d-41b4-bd02-d120d0d8d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set\n",
    "test_y_hat, test_y_ood_scores = classifier.predict(test_x)\n",
    "\n",
    "# Compute metrics\n",
    "recall_tumor, recall_stroma, recall_ood, avg_recall = compute_metrics(\n",
    "    y=test_y, y_hat=test_y_hat, ood_scores=test_y_ood_scores, threshold=threshold_val)\n",
    "\n",
    "# Display metrics\n",
    "print(f\"Tumor recall: {recall_tumor*100:.2f}%\")\n",
    "print(f\"Stroma recall: {recall_stroma*100:.2f}%\")\n",
    "print(f\"OoD recall: {recall_ood*100:.2f}%\")\n",
    "print(f\"Average recall: {avg_recall*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f0d345",
   "metadata": {},
   "source": [
    "## Part 2 - Lung Adenocarcinoma Classification (19 points)\n",
    "\n",
    "In the previous exercise, we successfully detected TUMOR and STROMA tissues using a minimum of labels. This allows us to compute the Tumor-Stroma Ratio (TSR), a valuable indicator for determining tumor grade and guiding treatment decisions. However, despite saving annotations, the need for hundreds of tumor/stroma annotations remains prohibitively expensive. Additionally, associating the TSR value with the correct tumor grade and treatment necessitates further algorithmic developments.\n",
    "\n",
    "An alternative approach involves annotating entire tumor grades on Whole Slide Images (WSIs) and training a classifier directly. However, a significant challenge arises due to the immense size of WSIs, typically containing millions of pixels, which makes direct preprocessing infeasible for computers. To address this challenge, we partition the WSI into thousands of non-overlapping patches. Consequently, each WSI comprises thousands of patch features. However, classifying a conglomerate of features is inherently challenging, especially considering that each WSI may not necessarily contain the same number of patches.\n",
    "\n",
    "As illustrated in Fig. 2, a highly effective method is to apply an aggregation pooling function, which transforms the pool of features into a single slide feature. Subsequently, we can train a simple classifier to classify the slides. In this section, your objective is to develop a classifier and various pooling methods for classifying WSIs of lung adenocarcinoma patterns.\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"../data/data_lab_03/part_02/wsi.png\" width=\"1100\">\n",
    "    <figcaption>Fig2: WSI classification pipeline.</figcaption>\n",
    "</figure>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13bf60f",
   "metadata": {},
   "source": [
    "### 2.1 Dataset (1 pt)\n",
    "\n",
    "Your objective is to classify lung adenocarcinoma patterns, specifically acinar and solid patterns. Acinar adenocarcinoma typically exhibits glandular structures resembling small sacs when viewed under a microscope, as depicted in Fig. 3. These structures may appear irregular and crowded. Conversely, solid adenocarcinoma, as illustrated in Fig. 4, appears as solid sheets or nests of cells with little to no glandular differentiation.\n",
    "\n",
    "You'll be working with a subset of the DHMC dataset, comprising 53 acinar examples and 48 solid examples. To assist you, we provide a training set and a validation set, representing 60% and 40% of the data, respectively. In this dataset, features of the patches composing each Whole Slide Image (WSI) have already been extracted using CTransPath. Your initial task is to prepare the data to be suitable for model training.\n",
    "\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"../data/data_lab_03/part_02/acinar.png\" width=\"500\">\n",
    "    <figcaption>Fig3: Example acinar lung adenocarcinoma WSI.</figcaption>\n",
    "</figure>\n",
    "<figure>\n",
    "    <img src=\"../data/data_lab_03/part_02/solid.png\" width=\"300\">\n",
    "    <figcaption>Fig4: Example solid lung adenocarcinoma WSI.</figcaption>\n",
    "</figure>\n",
    "<br />\n",
    "\n",
    "First download the data [here](https://drive.google.com/drive/folders/1W55290DTTj1ZketmSwLXbRQ-FEQrdees?usp=sharing) and put it under `data/data_lab_03/part_02`.\n",
    "Before proceeding, ensure that your workspace conforms to the following structure:\n",
    "\n",
    "```code\n",
    "├── lab_03_iapr.ipynb\n",
    "└── data\n",
    "    └── data_lab_03\n",
    "        └── part_02\n",
    "            ├── dhmc_train.pth\n",
    "            ├── dhmc_val.pth\n",
    "            ├── dhmc_test.pth\n",
    "            ├── acinar.png\n",
    "            ├── solid.png\n",
    "            ├── wsi.png\n",
    "            ├── DHMC_0001.png\n",
    "            └── DHMC_0007.png\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ef915-793e-4496-9119-911b34e1c29e",
   "metadata": {},
   "source": [
    "* **Q1 (1 pt)**: Please complete the `DHMC2Cls` class. The class loads raw data from an external file and stores them in `raw_data`. You are required to implement the `__len__` and `__getitem__` functions. The `__len__` function should return the length M of the dataset, while the `__getitem__` function should return a tuple containing: (1) the patch features, (2) the WSI label, (3) the WSI id, and (4) the patch coordinates. Before proceeding take time to investigate the content of the raw data. It's important to note that in the training mode, you should only return the features and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b020b-8df8-48de-a430-2e3affbfe2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DHMC2Cls(Dataset):\n",
    "    \"\"\"DHMC dataset using 2 classes\"\"\"\n",
    "\n",
    "    def __init__(self, features_path : str, train : bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            raw_data (list of dict): (M) List of M slides raw data as dictionaries. \n",
    "            train (bool): True if data are the training set. False otherwise\n",
    "            \n",
    "        Args:\n",
    "            features_path (str): The path to the features file\n",
    "            train (bool): Whether it is the training dataset or not\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        # Load raw data from path\n",
    "        self.raw_data = torch.load(features_path)\n",
    "        # Set if training or not\n",
    "        self.train = train\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length of the dataset\n",
    "\n",
    "        Returns:\n",
    "            int: The length M of the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        n_data = 0\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        \n",
    "        return n_data\n",
    "    \n",
    "    def __getitem__(self, index : int):\n",
    "        \"\"\"Returns the entry at index from the dataset\n",
    "\n",
    "        Args:\n",
    "            index (int): the requested entry index of the dataset\n",
    "\n",
    "        Returns:\n",
    "            features (torch.Tensor): (N, d) Feature tensor of the selected slide with N patches and d feature dimensions\n",
    "            label (int): Ground truth label {0, ..., n_classes}\n",
    "            wsi_id (str): Name of the WSI as \"DHMC_xxx\" where xxx is a unique id of the slide (train == False only)\n",
    "            coordinates (torch.Tensor): (N, 2) xy coordinates of the N patches of the selected slide (train == False only)\n",
    "        \"\"\"\n",
    "\n",
    "        features = None\n",
    "        label = None\n",
    "        wsi_id = None\n",
    "        coordinates = None\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        \n",
    "        if self.train:\n",
    "            return features, label\n",
    "        else:\n",
    "            return features, label, wsi_id, coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3baa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the block to build the train and validation datasets\n",
    "dataroot = \"../data/data_lab_03/part_02\"\n",
    "train_dataset = DHMC2Cls(os.path.join(dataroot, \"dhmc_train.pth\"), train=True)\n",
    "val_dataset = DHMC2Cls(os.path.join(dataroot, \"dhmc_val.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522af831-1900-4d51-9b3e-8c2970c133e0",
   "metadata": {},
   "source": [
    "In the cell below we test your training and validation datasets to check for inconsistencies. Your implementation should pass all tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f1eb2-3bec-464f-b47f-ac95a55a9e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_dataset(train_dataset: Dataset, val_dataset: Dataset):\n",
    "    \"\"\" Automatic check of implementation, DO NOT Modify\n",
    "\n",
    "    Args:\n",
    "        train_dataset (Dataset): Training dataset\n",
    "        val_dataset (Dataset): Validation dataset\n",
    "\n",
    "    Return:\n",
    "        status (str): Return \"Successful\" of all tests passed, \"Failed\" otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Perform sanity check of the training dataset creation\n",
    "        assert len(train_dataset) == 59\n",
    "        features, label = train_dataset[1]\n",
    "        assert label == 1\n",
    "        assert np.isclose(features[0, 0], 0.0538, rtol=1e-3)\n",
    "        \n",
    "        # Perform sanity check of the validation dataset creation\n",
    "        assert len(val_dataset) == 40\n",
    "        features, label, wsi_id, coord = val_dataset[1]\n",
    "        assert label == 1\n",
    "        assert np.isclose(features[0, 0], 0.0588, rtol=1e-3)\n",
    "        assert wsi_id == 'DHMC_0008'\n",
    "        assert coord[0, 0] == 21697\n",
    "        \n",
    "    except Exception:\n",
    "        return \"Failed :(\"\n",
    "\n",
    "    return \"Successful :)\"\n",
    "\n",
    "status = sanity_check_dataset(train_dataset, val_dataset)\n",
    "print(\"Satus of your implementation: {}\".format(status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e2dcb1-b631-419f-802b-9503193b05eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbcbe32",
   "metadata": {},
   "source": [
    "### 2.2 Average Pooling (1 pt)\n",
    "\n",
    "You will start with the simplest pooling method, i.e. average pooling. It simply consists of averaging the WSI patch features to form a single one representative of the WSI.\n",
    "\n",
    "* **Q1 (1 pt)**: Complete the `forward` function in `AveragePooling`. Remember, it takes a set of WSI features with shape `(N x d)`, and should return a single WSI feature of shape `(1 x d)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c30d28c-d569-4727-8d05-56a311e5172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AveragePooling(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, features : torch.Tensor):\n",
    "        \"\"\" Perform mean along the first dimension of the tensor\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): (N, D) Feature to perform average pooling on\n",
    "        Return:\n",
    "            mean (torch.Tensor): (1, D) Features average over all patches\n",
    "        \"\"\"\n",
    "\n",
    "        mean = None\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "        \n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fac6879-a1f7-41e6-b3ed-f5aee3a3e834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_avg(avg_func: Callable):\n",
    "    \"\"\" Automatic check of implementation, DO NOT Modify\n",
    "\n",
    "    Args:\n",
    "        avg_func (Dataset): Average pooling function\n",
    "\n",
    "    Return:\n",
    "        status (str): Return \"Successful\" of all tests passed, \"Failed\" otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Sanity check for average function\n",
    "        x_in = torch.Tensor([[0, 3], [4, 6], [2, 0]])\n",
    "        x_out = torch.Tensor([[2, 3]])\n",
    "        assert np.all(np.isclose(AveragePooling().forward(features=x_in), x_out, rtol=1e-3))\n",
    "        \n",
    "    except Exception:\n",
    "        return \"Failed :(\"\n",
    "\n",
    "    return \"Successful :)\"\n",
    "\n",
    "sanity_check_avg(AveragePooling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da1078",
   "metadata": {},
   "source": [
    "### 2.3 Classifier (7 pts)\n",
    "\n",
    "Now that you have coded your first aggregation method, let's build the linear classifier.\n",
    "\n",
    "* **Q1 (3 pts)**: \n",
    "    * Complete the `Classifier` class below. You should fill `__init__` which assigns the attributes. Attributes are :\n",
    "        * `proj` is a nonlinear projection layer that adapts the features for the task. It is simply a linear layer that projects features of dimension `d` to features of lower dimension `H`. Then it is followed by a ReLU.\n",
    "        * `pool`, the pooling function.\n",
    "        * `fc`, the final linear classifier layer. \n",
    "    * Complete `forward` which given a pool of features of shape `(1 x N x d)` outputs the class prediction logits of shape `(1 x 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137088df-a6b6-4066-9d15-abf7a1e95d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim : int, H : int, n_classes : int, pooling_fn : nn.Module) -> None:\n",
    "        \"\"\"Constructs the linear classifier\n",
    "\n",
    "        Attributes:\n",
    "            proj (Callable): Projection of layer (N, d) -> (N, H)\n",
    "            pool (Callable): Pooling layer (N, H) -> (1, H)\n",
    "            fc (Callable): Classification layer (1, H) -> (1, n_classes)\n",
    "            \n",
    "        Args:\n",
    "            in_dim (int): The dimension of input features\n",
    "            H (int): Target dimension for the projection layer\n",
    "            n_classes (int): The number of classes for the task\n",
    "            pooling_fn (nn.Module): The pooling function to aggregate the features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        proj_layer = None\n",
    "        pool_layer = None\n",
    "        fc_layer = None\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "        self.proj = proj_layer\n",
    "        self.pool = pool_layer\n",
    "        self.fc = fc_layer\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward path\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): (1, N, d) Input feature for a given slide with N patches\n",
    "        Return:\n",
    "            logits (torch.Tensor): (1, n_classes) Output logits for classification\n",
    "        \"\"\"\n",
    "\n",
    "        logits = None\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e19f40-3230-46b9-9819-dd248ecf7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check_cls(cls_class: Callable):\n",
    "    \"\"\" Automatic check of implementation, DO NOT Modify\n",
    "\n",
    "    Args:\n",
    "        avg_func (nn.Module): Linear classifier\n",
    "\n",
    "    Return:\n",
    "        status (str): Return \"Successful\" of all tests passed, \"Failed\" otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Test of implementation\n",
    "        d, H, n_classes = 768, 512, 2\n",
    "        cls = LinearClassifier(in_dim=d, H=H, n_classes=n_classes, pooling_fn=AveragePooling())\n",
    "        assert cls(torch.zeros((1, 1000, d))).shape == torch.Size([1, n_classes])\n",
    "        \n",
    "    except Exception:\n",
    "        return \"Failed :(\"\n",
    "\n",
    "    return \"Successful :)\"\n",
    "\n",
    "sanity_check_cls(LinearClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1c89ff",
   "metadata": {},
   "source": [
    "The classifier is ready to train. It remains to write the code to optimize your model.\n",
    "\n",
    "* **Q2 (2 pts)**: Please complete the `train` function below. This function should take `train_loader`, `val_loader`, `n_epochs`, and an `optimizer` as inputs. It is responsible for training the `model` and returning the best model checkpoint, best F1 score, and the epoch at which the best F1 score was achieved on the validation set.\n",
    "\n",
    "**Notes**: \n",
    "* Refer to this [tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) for guidance on training a classifier.\n",
    "* To obtain the model checkpoint, simply call `model.state_dict()`.\n",
    "* We provide you the `test` function that computes the F1 score on a given test dataset. **You should not modify it !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model : nn.Module, test_loader : DataLoader):\n",
    "    \"\"\"The test function, computes the F1 score of the current model on the test_loader\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to evaluate\n",
    "        test_loader (DataLoader): The test data loader to iterate on the dataset to test\n",
    "\n",
    "    Returns:\n",
    "        f1 (float): The F1 score on the given dataset\n",
    "        loss (float): Averaged loss on the given dataset\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    preds_dict = {\"preds\" : torch.Tensor(), \"labels\" : torch.Tensor(), 'losses': torch.Tensor()}\n",
    "    for features, labels, _, _ in test_loader:\n",
    "        # Forward and loss\n",
    "        preds = model(features)\n",
    "        loss = F.cross_entropy(preds, labels)\n",
    "        \n",
    "        # Store values\n",
    "        preds_dict[\"preds\"] = torch.cat([preds_dict[\"preds\"], preds.argmax(1)])\n",
    "        preds_dict[\"labels\"] = torch.cat([preds_dict[\"labels\"], labels])\n",
    "        preds_dict[\"losses\"] = torch.cat([preds_dict[\"losses\"], loss[None]])\n",
    "\n",
    "    # Compute metric and loss\n",
    "    f1 = f1_score(preds_dict[\"labels\"], preds_dict[\"preds\"], average=\"macro\")\n",
    "    loss = preds_dict[\"losses\"].mean()\n",
    "\n",
    "    return f1, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757eac4-d786-4b0f-a7cf-7aaf04b88293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model : nn.Module, train_loader : DataLoader, val_loader : DataLoader, n_epochs : int, optimizer : torch.optim.Optimizer):\n",
    "    \"\"\"Trains the neural network self.model for n_epochs using a given optimizer on the training dataset.\n",
    "    Outputs the best model in terms of F1 score on the validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The model to train\n",
    "        train_loader (DataLoader): The training dataloader to iterate on the training dataset\n",
    "        val_loader (DataLoader): The validation dataloader to iterate on the validation dataset\n",
    "        n_epochs (int): The number of epochs, i.e. the number of time the model should see each training example\n",
    "        optimizer (torch.optim.Optimizer): The optimizer function to update the model parameters\n",
    "\n",
    "    Returns:\n",
    "        best_model (nn.Module): Best model state dictionary \n",
    "        best_f1 (float): Best F1-score on the validation set\n",
    "        best_epoch (int): Best epoch on validation set\n",
    "        val_f1s (list of floats): (n_epochs, ) F1-scores for all epochs\n",
    "        val_losses (list of floats): (n_epochs, ) Losses for all validation epochs\n",
    "        train_losses(list of floats): (n_epochs, ) Losses for all training epochs\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variable to return\n",
    "    best_model = model.state_dict()\n",
    "    best_epoch = 0\n",
    "    best_f1 = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_f1s = []\n",
    "\n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return best_model, best_f1, best_epoch, val_f1s, val_losses, train_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c6225d-e7ff-4680-a92d-8b85ec244e35",
   "metadata": {},
   "source": [
    "* **Q3 (1 pt)**: Train a linear classifier using `AveragePooling` for `30` epochs, employing the `Adam` optimizer with a learning rate of `1e-3`. No need to search for optimal hyperparameters. Refer to the PyTorch documentation for guidance on constructing your optimizer. Use `H=512`. Don't worry, the training might take ~5-10 minutes. Your results should be above (87%).\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34334c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Display loss progression and f1 score\n",
    "epochs = 30\n",
    "d, H, n_classes = 768, 512, 2\n",
    "\n",
    "# Create Linear classifier and optimizer\n",
    "model = LinearClassifier(in_dim=d, H=H, n_classes=n_classes, pooling_fn=AveragePooling())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Run training and display results\n",
    "_, best_f1, best_epoch, val_accs, val_loss, train_loss = train(model, train_loader, val_loader, n_epochs=epochs, optimizer=optimizer)\n",
    "print(f\"Best model at epoch {best_epoch} -> {100*best_f1:.2f}% F1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d638e2-8c0a-4787-9dc8-550dc68a4fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(best_epoch: int, val_accs: list, val_loss: list, train_loss: list):\n",
    "    \"\"\"Plot training results of linear classifier\n",
    "    \n",
    "    Args:\n",
    "        best_epoch (int): Best epoch\n",
    "        val_accs (List): (E,) list of validation measures for each epoch\n",
    "        val_loss (List): (E,) List of validation losses for each epoch\n",
    "        train_loss (List): (E,) List of training losses for each epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Create plot\n",
    "    _, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    es = np.arange(1, len(val_accs)+1)\n",
    "    # Plot F1 score\n",
    "    axes[0].plot(es, val_accs, label=\"Val\")\n",
    "    axes[0].vlines(best_epoch, ymin=np.min(val_accs), ymax=np.max(val_accs), color='k', ls='--', label=\"Best epoch\")\n",
    "    axes[0].set_xlabel(\"Training steps\")\n",
    "    axes[0].set_ylabel(\"F1-score\")\n",
    "    axes[0].set_title(\"F1-score\")\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot losses\n",
    "    axes[1].plot(es, val_loss, label=\"Val\")\n",
    "    axes[1].plot(es, train_loss, label=\"Train\")\n",
    "    axes[1].vlines(best_epoch, ymin=np.min(train_loss), ymax=np.max(val_loss), color='k', ls='--', label=\"Best epoch\")\n",
    "    axes[1].set_xlabel(\"Training steps\")\n",
    "    axes[1].set_ylabel(\"Loss\")\n",
    "    axes[1].set_title(\"Losses\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot results\n",
    "plot_training(best_epoch=best_epoch, val_accs=val_accs, val_loss=val_loss, train_loss=train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad8d802-6d5a-4082-8bc0-6d33fdda432c",
   "metadata": {},
   "source": [
    "* **Q4 (1 pt)**: Are you satisfied with the results? What is the primary disadvantage of employing average pooling? Does the model overfit the data? (justify)\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b9919f",
   "metadata": {},
   "source": [
    "### 2.4 Attention Pooling (10 pts)\n",
    "\n",
    "Now you will build a more advanced pooling method, called attention pooling. The motivation for this method should result from your analysis in `2.3 Q4`. So we will not share much information with you on this one. Instead, we refer you to the related paper [here](https://arxiv.org/pdf/1802.04712.pdf).\n",
    "\n",
    "* **Q1 (4 pts)**: Complete `Attn_Net_Gated` which implements the gated attention mechanism described in the paper. Note `L`, and `M` are the dimension of the projection weights. You will find similar notations in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae9c7be-77d0-4dbe-9e16-fd4850004299",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn_Net_Gated(nn.Module):\n",
    "    def __init__(self, L : int, M : int):\n",
    "        \"\"\"\n",
    "        Attention Network with Sigmoid Gating (3 fc layers)\n",
    "        Args:\n",
    "            L: input feature dimension\n",
    "            M: hidden layer dimension\n",
    "        \"\"\"\n",
    "        super(Attn_Net_Gated, self).__init__()\n",
    "\n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward path of the gated attention network\n",
    "\n",
    "        Args:\n",
    "            xin: (N, L) List of N patches and L features\n",
    "        Return:\n",
    "            A: (N, 1) Attention value for each patch\n",
    "        \"\"\"\n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a35f4d-4371-4dc3-9b24-a3def7176571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_gated(func_gated: Callable):\n",
    "    \"\"\" Automatic check of implementation, DO NOT Modify\n",
    "\n",
    "    Args:\n",
    "        avg_func (nn.Module): Attention gated\n",
    "\n",
    "    Return:\n",
    "        status (str): Return \"Successful\" of all tests passed, \"Failed\" otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Test of implementation\n",
    "        L, M = 512, 256\n",
    "        layer = func_gated(L, M)\n",
    "        # Check output size\n",
    "        xin = torch.zeros(1000, L)\n",
    "        assert layer(xin).size() == torch.Size([1000, 1])\n",
    "    except Exception:\n",
    "        return \"Failed :(\"\n",
    "\n",
    "    return \"Successful :)\"\n",
    "\n",
    "sanity_gated(Attn_Net_Gated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a5140c",
   "metadata": {},
   "source": [
    "* **Q2 (3 pts)**: Complete `AttentionPooling`, which performs attention pooling with the help of the gated attention mechanism. In `forward`, you should only return the attention if `attention_only=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991c782-1385-45c3-8256-f8e678bb30a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionPooling(nn.Module):\n",
    "    def __init__(self, L : int, M : int):\n",
    "        super().__init__()\n",
    "        # Intatiate the gated layer\n",
    "        self.attention_net = Attn_Net_Gated(L, M)\n",
    "\n",
    "    def forward(self, x, attention_only : bool = False):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            x (torch.tensor): (N, L) Input feature over N patches and L features\n",
    "            attention_only (bool): Say whether to return the attention or not\n",
    "        Returns:\n",
    "            Y (torch.Tensor): (1, N) Output, if attention_only==False\n",
    "            A (torch.Tensor): (1, M) Attention values, if attention_only==True\n",
    "        \"\"\"\n",
    "\n",
    "        A = None\n",
    "        Y = None\n",
    "        \n",
    "        # ------------------\n",
    "        # Your code here ... \n",
    "        # ------------------\n",
    "\n",
    "        # Check if need to return attention\n",
    "        if attention_only:\n",
    "            return A\n",
    "        else:\n",
    "            return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0784cd",
   "metadata": {},
   "source": [
    "* **Q3 (1 pt)**: Train your linear classifier using `AttentionPooling` with `M=256`. You will train your model for `30` epochs, employing the `Adam` optimizer with a learning rate of `1e-4`. No need to search for optimal hyperparameters. Use `H=512`. Don't worry, the training should take ~5-10 minutes. Your results should be above (90%).\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf008582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Display loss progression and f1 score\n",
    "epochs = 30\n",
    "d, H, M, n_classes = 768, 512, 256, 2\n",
    "\n",
    "# Create Linear classifier and optimizer\n",
    "model = LinearClassifier(in_dim=d, H=H, n_classes=n_classes, pooling_fn=AttentionPooling(H, M))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Run training and display results\n",
    "best_model, best_f1, best_epoch, val_accs, val_loss, train_loss = train(model, train_loader, val_loader, n_epochs=epochs, optimizer=optimizer)\n",
    "print(f\"Best model at epoch {best_epoch} -> {100*best_f1:.2f}% F1 score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646a58df-1907-4c2b-9d3c-96e51e31f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plot_training(best_epoch=best_epoch, val_accs=val_accs, val_loss=val_loss, train_loss=train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0cb8e7",
   "metadata": {},
   "source": [
    "We have kept one example per class for testing and visualization. We have `DHMC_0001.jpg` presenting solid adenocarcinoma patterns and `DHMC_0007.jpg` with acinar adenocarcinoma. Those examples have never been seen in training and validation.\n",
    "\n",
    "* **Q4 (1 pt)**: Test your best attention model on the test dataset below. Use `load_state_dict()` to load the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2a0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = DHMC2Cls(os.path.join(dataroot, \"dhmc_test.pth\"))\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Create a model from the best model state\n",
    "model = LinearClassifier(in_dim=d, H=H, n_classes=n_classes, pooling_fn=AttentionPooling(H, M))\n",
    "model.load_state_dict(best_model)\n",
    "test_f1, _ = test(model, test_loader)\n",
    "\n",
    "print(f\"Test F1 score: {100*test_f1:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3151bbe2",
   "metadata": {},
   "source": [
    "As stated in the paper, a benefit of incorporating an attention layer is the enhanced interpretability of the model's decision-making process. This feature is particularly crucial for ensuring the safe deployment of deep learning models, especially in sensitive domains such as the medical field. With an attention layer, it becomes possible to discern the most critical patches that the model considered for a particular decision. To visualize this, please execute the three cells below to observe the attention maps on the test examples. **Don't forget to answer the question in the end !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67398f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prediction_map(\n",
    "        coords_x: np.ndarray,\n",
    "        coords_y:  np.ndarray,\n",
    "        feature:  np.ndarray,\n",
    "        wsi_dim: Optional[tuple] = None,\n",
    "        default: Optional[float] = -1.,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a prediction map based on x and y coordinates and feature vectors. Default values if feature is nonexisting\n",
    "    for a certain location is -1.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    coords_x: np.ndarray of shape (N,)\n",
    "        Coordinates of x points.\n",
    "    coords_y: np.ndarray of shape (N,)\n",
    "        Coordinates of y points.\n",
    "    feature: np.ndarray of shape (N, M)\n",
    "        Feature vector.\n",
    "    wsi_dim: tuple of int, optional\n",
    "        Size of the original whole slide.\n",
    "    default: float, optional\n",
    "        Value of the pixel when the feature is not defined.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    map: np.ndarray (W, H, M)\n",
    "        Feature map. The unaffected points use the default value -1.\n",
    "    map_x, map_y: np.ndarray (W, H), np.ndarray (W, H)\n",
    "        Corresponding and y coordinates of the feature map.\n",
    "    \"\"\"\n",
    "    # Compute offset of coordinates in pixel (patch intervals)\n",
    "    interval_x = np.min(np.unique(coords_x)[1:] - np.unique(coords_x)[:-1])\n",
    "    interval_y = np.min(np.unique(coords_y)[1:] - np.unique(coords_y)[:-1])\n",
    "\n",
    "    # Define new coordinates\n",
    "    offset_x = np.min(coords_x) % interval_x\n",
    "    offset_y = np.min(coords_y) % interval_y\n",
    "        \n",
    "    coords_x_ = ((coords_x - offset_x) / interval_x).astype(int)\n",
    "    coords_y_ = ((coords_y - offset_y) / interval_y).astype(int)\n",
    "\n",
    "    # Define size of the feature map\n",
    "    map = default * np.ones((int(wsi_dim[1] / interval_y), int(wsi_dim[0] / interval_x), feature.shape[1]))\n",
    "    map[coords_y_, coords_x_] = feature\n",
    "    \n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd025b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def plot_attention(model, test_loader):\n",
    "    \"\"\" Plot attention on top of slide images\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Model \n",
    "        test_loader (Dataloader): Data loader for the test set\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define new plot\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(16, 10), height_ratios=[3, 2], width_ratios=[1, 1.25])\n",
    "\n",
    "    # iterate over slides\n",
    "    for i, (features, _, wsi_id, coordinates) in enumerate(test_loader):\n",
    "\n",
    "        # Get data and paths\n",
    "        wsi_id = wsi_id[0]\n",
    "        slide_path = os.path.join(dataroot, f\"{wsi_id}.jpg\")\n",
    "        # Forward path\n",
    "        attention = model.pool(model.proj(features.squeeze()), attention_only=True).squeeze()\n",
    "\n",
    "        # Get WSI dim (Hardcoded)\n",
    "        if wsi_id == \"DHMC_0001\":\n",
    "            label = \"Solid\"\n",
    "            wsi_dim= (39839, 30468)\n",
    "        elif wsi_id == \"DHMC_0007\":\n",
    "            label = \"Acinar\"\n",
    "            wsi_dim = (47808, 22631)\n",
    "        else:\n",
    "            raise NotImplementedError(\"There is a problem !\")\n",
    "\n",
    "        # Plot results\n",
    "        slide_im = np.array(Image.open(slide_path))\n",
    "        ax[i][0].imshow(slide_im)\n",
    "        ax[i][0].set_title(label)\n",
    "        ax[i][0].axis('off')\n",
    "        \n",
    "        # Show prediction overlay\n",
    "        prob_map = build_prediction_map(\n",
    "                coords_x=coordinates[0,:, 0].numpy(),\n",
    "                coords_y=coordinates[0,:, 1].numpy(),\n",
    "                feature=attention[:, None],\n",
    "                wsi_dim=wsi_dim,\n",
    "                default=0,\n",
    "        )[:, :, 0]\n",
    "\n",
    "        # Rescale to ouput map size\n",
    "        prob_map = F.interpolate(torch.Tensor(prob_map)[None, None], slide_im.shape[:2], mode='bilinear', align_corners=False)[0, 0]\n",
    "\n",
    "        # Plot prediction map\n",
    "        ax[i][1].imshow(slide_im)\n",
    "        pcm = ax[i][1].imshow(prob_map, cmap=matplotlib.colormaps['hot'], vmax=torch.quantile(attention, q=0.99), alpha=0.5)\n",
    "        ax[i][1].axis('off')\n",
    "        # Add colorbar\n",
    "        fig.colorbar(pcm, ax=ax[i][1])\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16851826",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_attention(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdfe680",
   "metadata": {},
   "source": [
    "* **Q5 (1pt)**: From the visualization above, what can you interpret?\n",
    "    * **Answer**: ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
