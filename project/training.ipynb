{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import copy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from typing import Optional, Callable\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import json\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter # For Tensorboard\n",
    "from collections.abc import Mapping\n",
    "\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "torch.manual_seed(0)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoinDataset(Dataset):\n",
    "    \"\"\"DHMC dataset using 2 classes\"\"\"\n",
    "\n",
    "    def __init__(self, features_path : str, data_json, transform : bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            raw_data (list of dict): (M) List of M slides raw data as dictionaries. \n",
    "            train (bool): True if data are the training set. False otherwise\n",
    "            \n",
    "        Args:\n",
    "            features_path (str): The path to the features file\n",
    "            train (bool): Whether it is the training dataset or not\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.raw_data = []\n",
    "        # data_json = None\n",
    "        # with open(label_path, 'r') as f:\n",
    "        #     data_json = json.load(f)\n",
    "        for x in data_json:\n",
    "            filename = x[\"filename\"]\n",
    "            img_path = f\"{features_path}/{filename}\"\n",
    "            img = Image.open(img_path)\n",
    "            self.raw_data.append({\"image_features\": img, \"label\": int(x[\"value\"])})\n",
    "        \n",
    "        if transform:\n",
    "            self.transform = v2.Compose([\n",
    "                v2.Resize(224),\n",
    "                v2.CenterCrop(224),\n",
    "                v2.RandomRotation(degrees=(0, 300)),\n",
    "                v2.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.4, hue=0.3),\n",
    "                v2.ToTensor(),\n",
    "                v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                v2.RandomErasing(p=0.5)\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = v2.Compose([\n",
    "                v2.Resize(224),\n",
    "                v2.CenterCrop(224),\n",
    "                v2.ToTensor(),\n",
    "                v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the length of the dataset\n",
    "\n",
    "        Returns:\n",
    "            int: The length M of the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        n_data = len(self.raw_data)\n",
    "        return n_data\n",
    "    \n",
    "    def __getitem__(self, index : int):\n",
    "        \"\"\"Returns the entry at index from the dataset\n",
    "\n",
    "        Args:\n",
    "            index (int): the requested entry index of the dataset\n",
    "\n",
    "        Returns:\n",
    "            features (torch.Tensor): (N, d) Feature tensor of the selected slide with N patches and d feature dimensions\n",
    "            label (int): Ground truth label {0, ..., n_classes}\n",
    "            wsi_id (str): Name of the WSI as \"DHMC_xxx\" where xxx is a unique id of the slide (train == False only)\n",
    "            coordinates (torch.Tensor): (N, 2) xy coordinates of the N patches of the selected slide (train == False only)\n",
    "        \"\"\"\n",
    "\n",
    "        features = None\n",
    "        label = None\n",
    "\n",
    "        features = self.raw_data[index][\"image_features\"]\n",
    "        label = torch.tensor(self.raw_data[index][\"label\"])\n",
    "    \n",
    "        features = self.transform(features)\n",
    "        return features, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bluedrack/.conda/envs/iapr/lib/python3.9/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load all the img from output folder and make a dataset\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_json = None\n",
    "with open('data-fixed.json', 'r') as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "# final_data = []\n",
    "# for x in data_json:\n",
    "#     final_data.append({\"filename\": x[\"filename\"], \"value\": x[\"value\"]})\n",
    "train_data, test_data = train_test_split(data_json, test_size=0.2, random_state=42)\n",
    "train_dataset = CoinDataset('./output/', train_data, transform=True)\n",
    "test_dataset = CoinDataset('./output/', test_data, transform=False)\n",
    "\n",
    "# train_size = int(0.8 * len(coinDataset))\n",
    "# test_size = len(coinDataset) - train_size\n",
    "\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(coinDataset, [train_size, test_size])\n",
    "# val_coinDataset = CoinDataset('./output/', \"val_data_split.json\", transform=False)\n",
    "# trans = tranosforms.Compose([transforms.Resize(256),transforms.ToTensor()])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "# dataset = datasets.ImaVgeFolder('./output-1/', transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# model = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "# model.fc = nn.Sequential(\n",
    "#     nn.Dropout(0.5),\n",
    "#     nn.Linear(2048, 256),\n",
    "#     nn.BatchNorm1d(256),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(256, 16)\n",
    "# )\n",
    "\n",
    "# from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "# model.fc = nn.Sequential(\n",
    "#     nn.Dropout(0.5),\n",
    "#     nn.Linear(512, 256),\n",
    "#     nn.BatchNorm1d(256),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(256, 16)\n",
    "# )\n",
    "\n",
    "from torchvision.models import efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "\n",
    "\n",
    "\n",
    "model = efficientnet_v2_s(weights=EfficientNet_V2_S_Weights.IMAGENET1K_V1)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    # nn.Linear(1280, 256),\n",
    "    # nn.BatchNorm1d(256),\n",
    "    # nn.ReLU(),\n",
    "    nn.Linear(1280, 16)\n",
    ")\n",
    "# model.eval()\n",
    "# from model import ResNetModel\n",
    "# model = ResNetModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/305 (0%)]\tLoss: 2.807038, Acc: 6.250\n",
      "Val Epoch: 0\tLoss: 2.665947, Acc: 18.182, F1 micro: 0.182\n",
      "Train Epoch: 1 [0/305 (0%)]\tLoss: 2.768898, Acc: 6.250\n",
      "Val Epoch: 1\tLoss: 2.526230, Acc: 44.156, F1 micro: 0.442\n",
      "Train Epoch: 2 [0/305 (0%)]\tLoss: 2.637392, Acc: 9.375\n",
      "Val Epoch: 2\tLoss: 2.344788, Acc: 45.455, F1 micro: 0.455\n",
      "Train Epoch: 3 [0/305 (0%)]\tLoss: 2.453512, Acc: 34.375\n",
      "Val Epoch: 3\tLoss: 2.110254, Acc: 42.857, F1 micro: 0.429\n",
      "Train Epoch: 4 [0/305 (0%)]\tLoss: 2.208323, Acc: 40.625\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(model, train_loader, val_loader, optimizer, scheduler, criterion, epochs=10):\n",
    "    \n",
    "    model.to(device)\n",
    "    steps = 0\n",
    "    best_epoch = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for i, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc = accuracy_score(target, output.argmax(dim=1, keepdim=True))\n",
    "            writer.add_scalar(\"Acc/train\", acc, steps)\n",
    "            writer.add_scalar(\"Loss/train\", loss, steps)\n",
    "            steps+=1\n",
    "            if i % 10 == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Acc: {:.3f}'.format(\n",
    "                    epoch, i * len(data), len(train_loader.dataset),\n",
    "                    100. * i / len(train_loader), loss.item(), acc*100))\n",
    "                \n",
    "\n",
    "        running_vloss = []\n",
    "        val_f1micro = []\n",
    "        val_f1macro = []\n",
    "        correct = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vinputs, vlabels = vdata\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = criterion(voutputs, vlabels)\n",
    "                running_vloss.append(vloss)\n",
    "                pred = voutputs.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(vlabels.view_as(pred)).sum().item()\n",
    "                val_f1micro.append(f1_score(pred, vlabels, average='micro'))\n",
    "                val_f1macro.append(f1_score(pred, vlabels, average='macro'))\n",
    "\n",
    "        avg_vloss = np.mean(running_vloss)\n",
    "        avg_vf1micro = np.mean(val_f1micro)\n",
    "        avg_vf1macro = np.mean(val_f1macro)\n",
    "\n",
    "        val_accuracy = 100. * correct / len(val_loader.dataset)\n",
    "        print('Val Epoch: {}\\tLoss: {:.6f}, Acc: {:.3f}, F1 micro: {:.3f}'.format(\n",
    "            epoch, avg_vloss, val_accuracy, avg_vf1micro\n",
    "        ))\n",
    "\n",
    "        writer.add_scalar(\"Loss/val\", avg_vloss, (epoch + 1))\n",
    "        writer.add_scalar(\"Acc/val\", val_accuracy, (epoch + 1))\n",
    "        writer.add_scalar(\"F1_micro/val\", avg_vf1micro, epoch + 1)\n",
    "        writer.add_scalar(\"F1_macro/val\", avg_vf1macro, epoch + 1)\n",
    "        scheduler.step(avg_vloss)\n",
    "        if best_epoch < val_accuracy:\n",
    "            best_epoch = val_accuracy\n",
    "            torch.save(model, \"model/best_model.pth\")\n",
    "                \n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Define the scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "# Log in Tensorboard\n",
    "writer = SummaryWriter('runs/efficientnet_v2_s_baseline')\n",
    "\n",
    "train(model, train_loader, val_loader, optimizer, scheduler, nn.CrossEntropyLoss(), epochs=35)\n",
    "writer.flush()\n",
    "writer.close()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model, \"model/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 78.829MB\n"
     ]
    }
   ],
   "source": [
    "param_size = 0\n",
    "for param in model.parameters():\n",
    "    param_size += param.nelement() * param.element_size()\n",
    "buffer_size = 0\n",
    "for buffer in model.buffers():\n",
    "    buffer_size += buffer.nelement() * buffer.element_size()\n",
    "\n",
    "size_all_mb = (param_size + buffer_size) / 1024**2\n",
    "print('model size: {:.3f}MB'.format(size_all_mb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
